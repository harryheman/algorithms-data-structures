// Модель нано-нейрона.
// Она реализует базовую линейную зависимость между 'x' и 'y': y = w * x + b.
// Проще говоря, наш нейрончик - это "ребенок", умеющий рисовать прямую линию в системе координат XY.
// w, b - параметры модели.
class NanoNeuron {
  constructor(w, b) {
    // Нейрончику известны только эти 2 параметра линейной функции.
    // Эти параметры - то, что нейрончик будет учить в процессе обучения.
    this.w = w
    this.b = b
  }
  // Все, что умеет нейрончик, - имитировать линейную зависимость.
  // Он принимает некоторый 'x' и предсказывает 'y'. Никакой магии.
  predict(x) {
    return x * this.w + this.b
  }
}

// Конвертирует градусы Цельсия в градусы Фаренгейта по формуле: f = 1.8 * c + 32.
// Мы хотим научить нейрончик имитировать эту функцию (научить, что
// w = 1.8, а b = 32) без предоставления значений этих параметров.
// c - температура в градусах Цельсия
// f - вычисленная температура в градусах Фаренгейта
const W = 1.8
const B = 32
function celsiusToFahrenheit(c) {
  const f = c * W + B
  return f
}

// Генерирует обучающий и тестовый наборы данных с помощью функции celsiusToFahrenheit.
// Наборы данных состоят из пар входных значений и правильно размеченных выходных данных.
// В реальной жизни в большинстве случаев эти данные будут собраны а не сгенерированы.
// Например, у нас может быть набор изображений рукописных цифр и
// набор цифр, соответствующих цифре на изображении.
function generateDataSets() {
  // Генерируем ТРЕНИРОВОЧНЫЕ данные.
  // Эти данные будут использоваться для обучения модели.
  // Перед тем, как нейрончик вырастет и сможет принимать решения самостоятельно,
  // мы должны объяснить ему, что такое хорошо и что такое плохо с помощью
  // тренировочных данных.
  // xTrain -> [0, 1, 2, ...],
  // yTrain -> [32, 33.8, 35.6, ...]
  const xTrain = []
  const yTrain = []
  for (let x = 0; x < 100; x += 1) {
    const y = celsiusToFahrenheit(x)
    xTrain.push(x)
    yTrain.push(y)
  }

  // Генерируем ТЕСТОВЫЕ данные.
  // Эти данные будут использоваться для оценки того, насколько хорошо модель работает с данными,
  // которых она не видела в процессе обучения. Здесь мы можем увидеть,
  // что наш "ребенок" вырос и может принимать решения самостоятельно.
  // xTest -> [0.5, 1.5, 2.5, ...]
  // yTest -> [32.9, 34.7, 36.5, ...]
  const xTest = []
  const yTest = []
  // Начиная с 0.5 и используя такой же шаг 1, мы обеспечиваем
  // "непересекаемость" тренировочных и тестовых данных
  for (let x = 0.5; x < 100; x += 1) {
    const y = celsiusToFahrenheit(x)
    xTest.push(x)
    yTest.push(y)
  }

  return [xTrain, yTrain, xTest, yTest]
}

// Вычисляем стоимость (ошибку) между правильным выходным значением 'y' и 'prediction' (предсказанием), сделанным нейрончиком.
function predictionCost(y, prediction) {
  // Это просто разница между двумя значениями.
  // Чем ближе значения друг к другу, тем меньше разница.
  // Мы используем здесь степень 2 только для того, чтобы избавиться от отрицательных чисел,
  // т.е. (1 - 2) ^ 2 будет аналогичным (2 - 1) ^ 2.
  // Результат делится на 2 просто для упрощения дальнейшей формулы обратного распространения (см. ниже).
  return (y - prediction) ** 2 / 2 // например -> 235.6
}

// Прямое распространение.
// Эта функция берем все примеры из тренировочных наборов xTrain и yTrain и вычисляет
// предсказания модели для каждого примера из xTrain.
// По пути она также вычисляет стоимость предсказания (среднюю ошибку предсказаний нейрончика).
function forwardPropagation(model, xTrain, yTrain) {
  const m = xTrain.length
  const predictions = []
  let cost = 0
  for (let i = 0; i < m; i += 1) {
    const prediction = model.predict(xTrain[i])
    cost += predictionCost(yTrain[i], prediction)
    predictions.push(prediction)
  }
  // Нас интересует средняя стоимость.
  cost /= m
  return [predictions, cost]
}

// Обратное распространение.
// Здесь машинное обучение выглядит как магия.
// Ключевой концепцией здесь является производная (derivative), которая показывает, какой шаг нужно сделать, чтобы
// приблизиться к минимуму функции. Помните, нахождение минимальной функции стоимости -
// конечная цель процесса обучения. Функция стоимости выглядит следующим образом:
// (y - prediction) ^ 2 * 1/2, где prediction = x * w + b.
function backwardPropagation(predictions, xTrain, yTrain) {
  const m = xTrain.length
  // В начале мы не знаем, в каком направлении менять наши параметры 'w' и 'b'.
  // Поэтому мы устанавливаем шаги изменения дял каждого параметра в значение 0.
  let dW = 0
  let dB = 0
  for (let i = 0; i < m; i += 1) {
    // Это производная функции стоимости параметра 'w'.
    // Она показывает, в каком направлении (положительный/отрицательный знак 'dW') и
    // как быстро (абсолютное значение 'dW') параметр 'w' должен быть изменен.
    dW += (yTrain[i] - predictions[i]) * xTrain[i]
    // Это производная функции стоимости параметра 'b'.
    // Она показывает, в каком направлении (положительный/отрицательный знак 'dB') и
    // как быстро (абсолютное значение 'dB') параметр 'b' должен быть изменен
    dB += yTrain[i] - predictions[i]
  }
  // Нас интересуют средние дельты для каждого параметра.
  dW /= m
  dB /= m
  return [dW, dB]
}

// Обучает модель.
// Это "учитель" нашей модели:
// - он проводит некоторое время (epochs - эпохи) с нашим глупым нейрончиком и пытается тренировать/учить его,
// - он использует специальные "книги" (наборы данных xTrain и yTrain) для обучения,
// - он стимулирует нашего ребенка учиться сильнее (быстрее) с помощью параметра оценки обучения 'alpha'
// (чем сильнее стимул, тем быстрее модель учится, но если учитель будет давить слишком сильно
// у "ребенка" может случиться нервный срыв, и он больше не сможет учиться).
function trainModel(model, epochs, alpha, xTrain, yTrain) {
  // История обучения модели (массив).
  // Он может содержать хорошие или плохие "оценки" (стоимость), полученные в процессе обучения.
  const costHistory = []

  // Перебираем эпохи.
  for (let i = 0; i < epochs; i += 1) {
    // Прямое распространение для всех тренировочных примеров.
    // Сохраняем стоимость текущей итерации.
    // Это поможет анализировать обучение модели.
    const [predictions, cost] = forwardPropagation(model, xTrain, yTrain)
    costHistory.push(cost)

    // Обратное распространение. Учимся на ошибках.
    // Эта функция возвращает маленькие модификации, которые нужно применить к параметрам 'w' и 'b',
    // чтобы сделать предсказания более точными.
    const [dW, dB] = backwardPropagation(predictions, xTrain, yTrain)

    // Модифицируем параметры нейрончика для повышения точности предсказаний.
    nanoNeuron.w += alpha * dW
    nanoNeuron.b += alpha * dB
  }

  // Возвращаем историю обучения для анализа и визуализации.
  return costHistory
}

// ===
// Создаем экземпляр модели.
// В данный момент нейрончик не знает значений параметров 'w' и 'b'.
// Установим их произвольно.
const w = Math.random() // например -> 0.9492
const b = Math.random() // например -> 0.4570
const nanoNeuron = new NanoNeuron(w, b)

// Генерируем тренировочные и тестовые наборы данных.
const [xTrain, yTrain, xTest, yTest] = generateDataSets()

// Обучаем модели маленькими шагами (0.0005) в течение 70000 эпох.
// Можете попробовать другие значения, они определены эмпирическим путем.
const epochs = 70000
const alpha = 0.0005
const trainingCostHistory = trainModel(
  nanoNeuron,
  epochs,
  alpha,
  xTrain,
  yTrain,
)

// Проверим, как менялась стоимость в процессе обучения.
// Мы ожидаем, что стоимость после обучения будет значительно ниже, чем в начале.
// Это будет означать, что наш нейрончик становится умнее. Но возможно и обратное.
console.log('Стоимость до обучения:', trainingCostHistory[0]) // например -> 4694.3335043
console.log('Стоимость после обучения:', trainingCostHistory[epochs - 1]) // например -> 0.0000024

// Взглянем на параметры нейрончика, чтобы увидеть, чему он научился.
// Мы ожидаем, что значения параметров 'w' и 'b' модели будут близкими к значениям параметров 'W' и 'B',
// которые используются в функции celsiusToFahrenheit() (w = 1.8 и b = 32), коль скоро нейрончик пытается имитировать эту функцию.
console.log(
  'Параметры нейрончика:',
  JSON.stringify({ w: nanoNeuron.w, b: nanoNeuron.b }, null, 2),
) // например -> {w: 1.8, b: 31.99}

// Оцениваем точность модели на тестовых данных, чтобы увидеть, насколько хорошо модель справляется с неизвестными данными.
// Мы ожидаем, что стоимость предсказаний на тестовых данных будет близкой к стоимости предсказаний на тренировочных данных.
// Это будет означать, что нейрончик хорошо справляется как с тренировочными, так и с тестовыми данными.
const [testPredictions, testCost] = forwardPropagation(nanoNeuron, xTest, yTest)
console.log('Стоимость на новых тестовых данных:', testCost) // например -> 0.0000023

// После того, как "ребенок" хорошо показал себя в "школе" в процессе обучения и хорошо справился с тестовыми данными,
// мы можем назвать его "умным" и задать ему несколько вопросов.
const tempInCelsius = 70
const customPrediction = nanoNeuron.predict(tempInCelsius)
console.log(
  `Нейрончик "думает", что ${tempInCelsius}°C в градусах Фаренгейта:`,
  customPrediction,
) // -> 158.0002
console.log('Правильный ответ:', celsiusToFahrenheit(tempInCelsius)) // -> 158

// Очень близко! Как и люди, нейрончик хорош, но не идеален :)
// Счастливого обучения!
